# Imports

```py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
```

# Reading

```py
# read file into pandas using a relative path
df = pd.read_csv("file.csv", encoding='utf-8')
```

# EDA

```py
df.describe()
```

```py
df.groupby('label').describe()
```

```py
# convert label to a numerical variable
df['label_num'] = df.label.map({'label1' : 0, 'label2':1})
```

```py
# feature engineering
df['text_len'] = df.text.apply(len)
```

```py
# plot length
plt.figure(figsize=(12, 8))
df[df.label == 'label1'].text_len.plot(bins=35, kind='hist', color='blue', label='label1', alpha=0.6)
df[df.label == 'label2'].text_len.plot(kind='hist', color='red', label='label2', alpha=0.6)
plt.legend()
plt.xlabel("Length")
```

```py
df[df.label == 'label1'].describe()
```

# Text Pre-processing

Convert text into vectors (sequence of numbers).
- Remove punctuation (built-in string library).
- Remove common words (`NLTK` library).
- Split a message into its individual words and return a list.

```py
import string
from nltk.corpus import stopwords

def text_process(mess):
    """
    Takes in a string of text, then performs the following:
    1. Remove all punctuation
    2. Remove all stopwords
    3. Returns a list of the cleaned text
    """
    STOPWORDS = stopwords.words('english') + ['u', 'Ã¼', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']
    # Check characters to see if they are in punctuation
    nopunc = [char for char in mess if char not in string.punctuation]

    # Join the characters again to form the string.
    nopunc = ''.join(nopunc)
    
    # Now just remove any stopwords
    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])
```

```py
df['clean_text'] = df.message.apply(text_process)
```

```py
from collections import Counter

words = df[df.label=='label1'].clean_text.apply(lambda x: [word.lower() for word in x.split()])
label1_words = Counter()

for msg in words:
    label1_words.update(msg)
    
print(label1_words.most_common(50))
```

# Vectorization

Currently, we hace the messages as lists of tokens (**lemmas**) and now we need to convert each of those messages into a vector.
1. Count how many times does a word occur in each message (term frequency).
    - Each vector will have as many dimensions as there are unique words in the message corpus. We will first use SciKit Learn's `CountVectorizer`. This model will convert a collection of text documents to a matrix of token counts.
    - We can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message.
    - Since there are so many messages, we can expect a lot of zero counts for the presence of that word in that document. Because of this, SciKit Learn will output a Sparse Matrix.
3. Weight the counts, so that frequent tokens get lower weight (inverse document frequency).
4. Normalize the vectors to unit length, to abstract from the original text length (L2 norm).


```py
# split X and y into training and testing sets 
from sklearn.model_selection import train_test_split

# how to define X and y (from the df data) for use with COUNTVECTORIZER
X = df.clean_text
y = df.label_num
print(X.shape)
print(y.shape)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
```

Parameters for CountVectorizer:
- `stop_words`: string, list or None (default)
  - if 'english', a built-in stop word list for English is used.
  - if a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.
  - if None, no stop words will be used.
- `ngram_range`: tuple (min_n, max_n), default = (1,1)
  - The lower and upper boundaary of the range of n-values for different n-grams to be extracted.
  - All values of `n` such that min_n <= n <= max_n will be used.
- `max_df`: float in range [0.0, 1.0] or int, default = 1.0
  - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).
  - If float, the parameter represents a proportion of documents.
  - If integer, the parameter represents an absolute count.
- min_df: float in range [0.0, 1.0] or int, default=1
  - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called "cut-off" in the literature.)
  - If float, the parameter represents a proportion of documents.
  - If integer, the parameter represents an absolute count.

```py
from sklearn.feature_extraction.text import CountVectorizer

# instantiate the vectorizer
vect = CountVectorizer()
vect.fit(X_train)

# learn training data vocabulary, then use it to create a document-term matrix
X_train_dtm = vect.transform(X_train)

# equivalently: combine fit and transform into a single step
X_train_dtm = vect.fit_transform(X_train)

# examine the document-term matrix
print(type(X_train_dtm), X_train_dtm.shape)

# transform testing data (using fitted vocabulary) into a document-term matrix
X_test_dtm = vect.transform(X_test)
print(type(X_test_dtm), X_test_dtm.shape)
```

```py
from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer()
tfidf_transformer.fit(X_train_dtm)
tfidf_transformer.transform(X_train_dtm)
```

# Building and evaluating a model

## Multinomial Naive Bayes

The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.

```py
# import and instantiate a Multinomial Naive Bayes model
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
```

```py
# train the model using X_train_dtm (timing it with an IPython "magic command")
%time nb.fit(X_train_dtm, y_train)
```

```py
from sklearn import metrics

# make class predictions for X_test_dtm
y_pred_class = nb.predict(X_test_dtm)

# calculate accuracy of class predictions
print("=======Accuracy Score===========")
print(metrics.accuracy_score(y_test, y_pred_class))

# print the confusion matrix
print("=======Confision Matrix===========")
metrics.confusion_matrix(y_test, y_pred_class)
```

```py
# print message text for false positives
# X_test[(y_pred_class==1) & (y_test==0)]
X_test[y_pred_class > y_test]
```

```py
# print message text for false negatives
X_test[y_pred_class < y_test]
```

```py
# calculate predicted probabilities for X_test_dtm (poorly calibrated)
y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]
y_pred_prob
```

```py
# calculate AUC
metrics.roc_auc_score(y_test, y_pred_prob)
```

```py
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline

pipe = Pipeline([('bow', CountVectorizer()), 
                 ('tfid', TfidfTransformer()),  
                 ('model', MultinomialNB())])

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)

# calculate accuracy of class predictions
print("=======Accuracy Score===========")
print(metrics.accuracy_score(y_test, y_pred))

# print the confusion matrix
print("=======Confision Matrix===========")
metrics.confusion_matrix(y_test, y_pred)
```


## Logistic regression

Logistic regression is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.


```py
# import an instantiate a logistic regression model
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(solver='liblinear')

# train the model using X_train_dtm
%time logreg.fit(X_train_dtm, y_train)
```

```py
# make class predictions for X_test_dtm
y_pred_class = logreg.predict(X_test_dtm)

# calculate predicted probabilities for X_test_dtm (well calibrated)
y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]
y_pred_prob
```


```py
# calculate accuracy of class predictions
print("=======Accuracy Score===========")
print(metrics.accuracy_score(y_test, y_pred_class))

# print the confusion matrix
print("=======Confision Matrix===========")
print(metrics.confusion_matrix(y_test, y_pred_class))

# calculate AUC
print("=======ROC AUC Score===========")
print(metrics.roc_auc_score(y_test, y_pred_prob))
```




```py

```
